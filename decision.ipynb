{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFl6kX-JeKdG"
      },
      "source": [
        "## Установка необходимых библиотек"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOQ1BxroT6-s",
        "outputId": "c807d2a9-2190-4493-b0f7-edfb6fc2b784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE1RDE13yXsw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0738d763-3f5b-46ba-8e1c-7a1c08e0cf48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-dotenv==0.19.0\n",
            "  Downloading python_dotenv-0.19.0-py2.py3-none-any.whl (17 kB)\n",
            "Collecting tqdm==4.62.2\n",
            "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Collecting Pillow==7.0.0\n",
            "  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 29.6 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.4.3\n",
            "  Downloading matplotlib-3.4.3-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: opencv-python-headless==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.3) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.4.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.3) (1.15.0)\n",
            "Installing collected packages: Pillow, tqdm, python-dotenv, matplotlib\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-7.0.0 matplotlib-3.4.3 python-dotenv-0.19.0 tqdm-4.62.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1 in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 39.0 MB/s \n",
            "\u001b[?25hCollecting segmentation-models-pytorch==0.2.0\n",
            "  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting albumentations==1.0.3\n",
            "  Downloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.13.1) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.13.1) (2.23.0)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 61.0 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (6.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.3) (0.18.3)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.62.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (3.4.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.13.1) (1.24.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12422 sha256=4da03d2ce9d76897b1850558ad50b751911707e966b60476532bdde4da8db540\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=9a90fbb4a7ae439caf2585fdfbe258d088631c178c13db7ce3f42603624bc859\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch, scikit-learn, pytorch-ignite, albumentations\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed albumentations-1.0.3 efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 pytorch-ignite-0.4.10 scikit-learn-0.24.2 segmentation-models-pytorch-0.2.0 timm-0.4.12\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv==0.19.0 tqdm==4.62.2 numpy Pillow==7.0.0 matplotlib==3.4.3 opencv-python==4.6.0.66 opencv-python-headless==4.6.0.66 matplotlib\n",
        "!pip install scikit-learn==0.24.2 torch==1.12.1 torchvision==0.13.1 pytorch-ignite segmentation-models-pytorch==0.2.0 albumentations==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8hpHLWYv9Ak"
      },
      "outputs": [],
      "source": [
        "## Импорт необходимых библиотек\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Loss, Metric\n",
        "from ignite.engine import _prepare_batch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Callable, Tuple, Dict, Any, List, Sequence, Iterator, Optional\n",
        "from collections import defaultdict\n",
        "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FastRCNNPredictor\n",
        "\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сегментация дверей и окон"
      ],
      "metadata": {
        "id": "8Dazi4VbOjZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вспомогательные функции"
      ],
      "metadata": {
        "id": "nrRA5XNbN7xL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFuViSg26sA2"
      },
      "outputs": [],
      "source": [
        "## PLRC DATASET - класс и необходимые функции для датасета\n",
        "\n",
        "def get_color_map():\n",
        "    return {\n",
        "        #\"wall\": 255,\n",
        "        \"window\": 255\n",
        "    }\n",
        "\n",
        "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
        "    image = np.moveaxis(image, -1, 0)\n",
        "    image = np.ascontiguousarray(image)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def tensor_from_mask_image(mask: np.ndarray) -> torch.Tensor:\n",
        "    if len(mask.shape) == 2:\n",
        "        mask = np.expand_dims(mask, -1)\n",
        "    return tensor_from_rgb_image(mask)\n",
        "\n",
        "\n",
        "class PLRCDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform, start_index, end_index, mask_folder=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = PLRCDataset.parse_folder(self.image_folder, start_index, end_index)\n",
        "        self.color_map = get_color_map()\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_folder(path, start, end):\n",
        "        if path is None:\n",
        "            return []\n",
        "        images = glob.glob1(path,  '*.png')\n",
        "        images.sort()\n",
        "\n",
        "        return images[start:end]\n",
        "\n",
        "    @staticmethod\n",
        "    def load_image(path) -> np.array:\n",
        "        return cv2.imread(path, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_mask(path) -> np.array:\n",
        "        return cv2.imread(path, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_grayscale_mask_into_channels_by_color_map(mask, color_map) -> torch.Tensor:\n",
        "        masks = []\n",
        "\n",
        "        for i in color_map.values():\n",
        "            masks.append(mask == i)\n",
        "\n",
        "        return torch.cat(masks).float()\n",
        "\n",
        "    def mask_to_grayscale(self, masks) -> np.ndarray:\n",
        "        masks = masks.cpu().numpy()\n",
        "\n",
        "        colors_by_index = list(self.color_map.values())\n",
        "        img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
        "\n",
        "        for i in range(len(masks)):\n",
        "            img[masks[i] == 1] = colors_by_index[i]\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.images[index]\n",
        "        image_path = os.path.join(self.image_folder, image_name)\n",
        "\n",
        "        image = PLRCDataset.load_image(image_path)\n",
        "\n",
        "        if self.mask_folder is None:\n",
        "            # sample = self.transform(image=image)\n",
        "            # image = sample['image']\n",
        "            return image_name, tensor_from_mask_image(image).float() / 255.0\n",
        "\n",
        "        mask_path = os.path.join(self.mask_folder, image_name)\n",
        "        mask = PLRCDataset.load_mask(mask_path)\n",
        "\n",
        "        sample = self.transform(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        image = tensor_from_mask_image(image)\n",
        "        image = torch.cat([image, image, image])\n",
        "        mask = tensor_from_mask_image(mask)\n",
        "\n",
        "        mask = PLRCDataset.split_grayscale_mask_into_channels_by_color_map(mask, self.color_map)\n",
        "\n",
        "        return image.float() / 255.0, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "## PLRC UTILS = утилиты для подсчета\n",
        "\n",
        "\n",
        "def get_training_augmentation():\n",
        "    return A.Compose([\n",
        "        # A.RandomCrop(height=256, width=256, p=1),\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "        A.VerticalFlip(p=1),              \n",
        "        A.RandomRotate90(p=1)\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "def get_test_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ], p=1)\n",
        "\n",
        "\n",
        "def get_data_loader(path, batch_size, n_processes, start_index, end_index, shuffle=True):\n",
        "    image_path = os.path.join(path, 'image')\n",
        "    mask_path = os.path.join(path, 'mask')\n",
        "\n",
        "    dataset = PLRCDataset(image_folder=image_path, mask_folder=mask_path, transform=get_training_augmentation(), \n",
        "                          start_index=start_index, end_index=end_index)\n",
        "\n",
        "    return DataLoader(dataset=dataset, batch_size=batch_size, drop_last=True, num_workers=n_processes, shuffle=shuffle)\n",
        "\n",
        "\n",
        "def get_train_validation_data_loaders(path, batch_size, n_processes, train_split):\n",
        "    files_count = len(os.listdir(os.path.join(path, 'image')))\n",
        "\n",
        "    train_dl = get_data_loader(path, batch_size, n_processes, shuffle=True, start_index=0, end_index=int(files_count*train_split))\n",
        "    test_dl = get_data_loader(path, batch_size, n_processes, shuffle=False, start_index=int(\n",
        "        files_count*train_split), end_index=(files_count-1))\n",
        "\n",
        "    return train_dl, test_dl\n",
        "\n",
        "\n",
        "## DATA LOSS - функции для подсчета метрик качества обучения нейросетей\n",
        "\n",
        "class BCESoftDiceLoss:\n",
        "    def __init__(self, dice_weight=0):\n",
        "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
        "        self.dice_weight = dice_weight\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_dice(predict, target):\n",
        "        eps = 1e-15\n",
        "        batch_size = target.size()[0]\n",
        "\n",
        "        dice_target = (target == 1).float().view(batch_size, -1)\n",
        "        dice_predict = torch.sigmoid(predict).view(batch_size, -1)\n",
        "\n",
        "        inter = torch.sum(dice_predict * dice_target) / batch_size\n",
        "        union = (torch.sum(dice_predict) + torch.sum(dice_target)) / batch_size + eps\n",
        "\n",
        "        return (2 * inter.float() + eps) / union.float()\n",
        "\n",
        "    def __call__(self, predict, target):\n",
        "        loss = (1.0 - self.dice_weight) * self.nll_loss(predict, target)\n",
        "\n",
        "        if self.dice_weight:\n",
        "            loss -= self.dice_weight * torch.log(BCESoftDiceLoss.soft_dice(predict, target))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MultiClassBCESoftDiceLoss:\n",
        "    def __init__(self, dice_weight=0):\n",
        "        self.bce_soft_dice = BCESoftDiceLoss(dice_weight)\n",
        "\n",
        "    def __call__(self, predict, target):\n",
        "        classes = target.shape[1]\n",
        "        loss = predict.new_zeros(1)\n",
        "\n",
        "        for i in range(classes):\n",
        "            loss += self.bce_soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
        "\n",
        "        return loss[0] / float(classes)\n",
        "\n",
        "\n",
        "class MultiClassSoftDiceMetric(Metric):\n",
        "    def __init__(self):\n",
        "        super(MultiClassSoftDiceMetric, self).__init__()\n",
        "        self.general_loss = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.general_loss = 0\n",
        "\n",
        "    def update(self, output):\n",
        "        predict, target = output\n",
        "\n",
        "        classes = target.shape[1]\n",
        "        loss = predict.new_zeros(1)\n",
        "\n",
        "        for i in range(classes):\n",
        "            loss += BCESoftDiceLoss.soft_dice(predict[:, i].unsqueeze(1), target[:, i].unsqueeze(1))\n",
        "\n",
        "        self.general_loss = loss[0] / float(classes)\n",
        "\n",
        "    def compute(self):\n",
        "        return self.general_loss\n",
        "\n",
        "\n",
        "\n",
        "## Функции для обучения\n",
        "\n",
        "def load_trained_model(model, optimizer, model_path, optimizer_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "    print('Load model from: ', model_path)\n",
        "    print('Load optimizer from: ', optimizer_path)\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, model_path, optimizer_path, postfix='_'):\n",
        "    torch.save(model.state_dict(), model_path + postfix)\n",
        "    torch.save(optimizer.state_dict(), optimizer_path + postfix)\n",
        "\n",
        "\n",
        "def log_image(image, prefix, epoch, step):\n",
        "    img = Image.fromarray(image)\n",
        "    image_name = \"%s_%s_%s.png\" % (epoch, step, prefix)\n",
        "    img.save(image_name)\n",
        "\n",
        "    os.remove(image_name)\n",
        "\n",
        "\n",
        "def run_test_model(model, evaluate_loader, epoch, device, step=10):\n",
        "    model.eval()\n",
        "    count_step = 0\n",
        "\n",
        "    for idx, batch in enumerate(evaluate_loader):\n",
        "        if count_step > step:\n",
        "            break\n",
        "\n",
        "        x, y = _prepare_batch(batch, device)\n",
        "\n",
        "        predict = model(x)\n",
        "        predict = torch.sigmoid(predict) > 0.2\n",
        "\n",
        "        count_step += len(x)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def run_train(dataset_path, batch_size, n_processes, model_path, optimizer_path, load_pre_model=False,\n",
        "              device='cpu', lr=0.0001, betas=(0.9, 0.99), weight_decay=0.0004, epochs=10,\n",
        "              log_interval=20, save_interval=2, train_split=0.85):\n",
        "\n",
        "    train_loader, evaluate_loader = get_train_validation_data_loaders(path=dataset_path, batch_size=batch_size,\n",
        "                                                                      n_processes=n_processes, train_split=train_split)\n",
        "    model = smp.FPN('resnet50', classes=24)\n",
        "\n",
        "    if device.startswith('cuda'):\n",
        "        if not torch.cuda.is_available():\n",
        "            raise ValueError('CUDA is not available')\n",
        "\n",
        "        model = model.to(device)\n",
        "        print('CUDA is used')\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "\n",
        "    if load_pre_model:\n",
        "        load_trained_model(model, optimizer, model_path, optimizer_path)\n",
        "\n",
        "    trainer = create_supervised_trainer(model, optimizer, MultiClassBCESoftDiceLoss(0.7), device=device)\n",
        "    evaluator = create_supervised_evaluator(model,\n",
        "                                            metrics={'dice': MultiClassSoftDiceMetric(),\n",
        "                                                     'nll': Loss(MultiClassBCESoftDiceLoss(0.7))},\n",
        "                                            device=device)\n",
        "\n",
        "    desc = \"ITERATION - loss: {:.2f}\"\n",
        "    pbar = None\n",
        "\n",
        "    @trainer.on(Events.EPOCH_STARTED)\n",
        "    def create_pbar(engine):\n",
        "        model.train()\n",
        "        nonlocal pbar\n",
        "        pbar = tqdm(\n",
        "            initial=0, leave=False, total=len(train_loader),\n",
        "            desc=desc.format(0)\n",
        "        )\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_training_results(engine):\n",
        "        pbar.close()\n",
        "        evaluator.run(evaluate_loader)\n",
        "        metrics = evaluator.state.metrics\n",
        "        avg_dice = metrics['dice']\n",
        "        avg_nll = metrics['nll']\n",
        "\n",
        "\n",
        "        print(\"Training Results - Epoch: {}  Dice: {:.2f} Avg loss: {:.2f}\"\n",
        "              .format(engine.state.epoch, avg_dice, avg_nll))\n",
        "\n",
        "        if engine.state.epoch % save_interval == 0:\n",
        "            save_model(model, optimizer, model_path, optimizer_path, '_' + list(get_color_map().keys())[0]) #str(engine.state.epoch))\n",
        "            run_test_model(model, evaluate_loader, engine.state.epoch, device)\n",
        "\n",
        "    @trainer.on(Events.ITERATION_COMPLETED)\n",
        "    def log_training_loss(engine):\n",
        "\n",
        "        pbar.desc = desc.format(engine.state.output)\n",
        "        pbar.update()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    trainer.run(train_loader, max_epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "Ksgzck03RAHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "84fb5cbd-566c-4dce-dff2-81123c4d9ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.4.3\n",
            "    Uninstalling matplotlib-3.4.3:\n",
            "      Successfully uninstalled matplotlib-3.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrIseOYL6x94"
      },
      "outputs": [],
      "source": [
        "## Необходимые вспомогательные функции\n",
        "def get_test_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ], p=1)\n",
        "\n",
        "def mask_to_grayscale(masks) -> np.ndarray:\n",
        "    masks = masks.cpu().numpy()\n",
        "\n",
        "    img = np.zeros(masks.shape[1:], dtype=np.uint8)\n",
        "\n",
        "    img[masks[0] == 1] = 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n",
        "    image = np.moveaxis(image, -1, 0)\n",
        "    image = np.ascontiguousarray(image)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "def convert_image(img):\n",
        "    if img is None:\n",
        "        return\n",
        "    height, width, channels = img.shape\n",
        "    rect = (0, 0, 512, 512)\n",
        "    h = rect[3]\n",
        "    w = int(h * width / height)\n",
        "    if w > rect[2]:\n",
        "        w = rect[2]\n",
        "        h = int(height / width * w)\n",
        "    new_img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n",
        "    s = np.full((rect[3], rect[2], 3), np.uint8(255))\n",
        "    sy = int((rect[3] - h) / 2)\n",
        "    sx = int((rect[2] - w) / 2)\n",
        "    s[sy:sy + h, sx:sx + w] = new_img\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_map():\n",
        "    return {\n",
        "        \"wall\": 255,\n",
        "        \"window\": 255\n",
        "    }"
      ],
      "metadata": {
        "id": "i0ucH1I9yliv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VH_0TWXmYS0"
      },
      "source": [
        "## Детекция дверей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnIyPcTgle1F"
      },
      "source": [
        "### Вспомогательные функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klMp7gYWmDck"
      },
      "source": [
        "#### Инициализация модели обучения и функций для подсчета метрик качества обучения нейросетей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0D06LoHmQP2"
      },
      "outputs": [],
      "source": [
        "def initialize_model(num_classes: int,\n",
        "                     min_size: int, max_size: int,\n",
        "                     image_mean: Sequence[float], image_std: Sequence[float],\n",
        "                     device: str = None) -> torch.nn.Module:\n",
        "    pretrained_model = fasterrcnn_resnet50_fpn(pretrained=False, min_size=min_size, max_size=max_size,\n",
        "                                               image_mean=image_mean, image_std=image_std)\n",
        "    num_predictor_features = pretrained_model.roi_heads.box_head.fc7.out_features\n",
        "    pretrained_model.roi_heads.box_predictor = FastRCNNPredictor(num_predictor_features, num_classes)\n",
        "\n",
        "    return pretrained_model.to(device)\n",
        "\n",
        "\n",
        "def compute_iou(box1: torch.tensor, box2: torch.tensor) -> torch.tensor:\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    if x2 - x1 < 0 or y2 - y1 < 0:\n",
        "        return 0\n",
        "\n",
        "    intersection = (x2 - x1) * (y2 - y1)\n",
        "    sum_ = ((box1[2] - box1[0]) * (box1[3] - box1[1]) +\n",
        "            (box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
        "\n",
        "    iou = intersection / (sum_ - intersection)\n",
        "\n",
        "    return iou.item()\n",
        "\n",
        "\n",
        "class MeanAveragePrecision:\n",
        "\n",
        "    def __init__(self, num_conf: int = 11):\n",
        "        self.num_conf = num_conf\n",
        "\n",
        "    @staticmethod\n",
        "    def is_true(box: torch.tensor, label: int,\n",
        "                y_true: Dict[str, torch.tensor],\n",
        "                iou_thresh: float = 0.5) -> bool:\n",
        "        num_true = len(y_true['boxes'])\n",
        "\n",
        "        for i in range(num_true):\n",
        "            if label == y_true['labels'][i]:\n",
        "                if compute_iou(box, y_true['boxes'][i]) > iou_thresh:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_map_given_conf(y_preds: List[Dict[str, torch.tensor]],\n",
        "                               y_trues: List[Dict[str, torch.tensor]],\n",
        "                               conf: float) -> float:\n",
        "        num_pos = defaultdict(int)\n",
        "        num_true_pos = defaultdict(int)\n",
        "\n",
        "        for y_pred, y_true in zip(y_preds, y_trues):\n",
        "\n",
        "            num_pred = len(y_pred['boxes'])\n",
        "\n",
        "            for i in range(num_pred):\n",
        "                if y_pred['scores'][i] > conf:\n",
        "                    label = y_pred['labels'][i]\n",
        "                    num_pos[label] += 1\n",
        "\n",
        "                    if MeanAveragePrecision.is_true(y_pred['boxes'][i], y_pred['labels'][i], y_true):\n",
        "                        num_true_pos[label] += 1\n",
        "\n",
        "        all_classes_sum = sum(num_true_pos[lbl] / num_pos[lbl] for lbl in num_pos)\n",
        "\n",
        "        try:\n",
        "            precision = all_classes_sum / len(num_pos)\n",
        "        except ZeroDivisionError:\n",
        "            precision = 0\n",
        "\n",
        "        return precision\n",
        "\n",
        "    def __call__(self, y_pred: List[Dict[str, torch.tensor]],\n",
        "                 y_true: List[Dict[str, torch.tensor]]) -> float:\n",
        "        assert len(y_pred) == len(y_true)\n",
        "\n",
        "        map_sum = 0\n",
        "\n",
        "        for conf in np.linspace(0, 1, self.num_conf):\n",
        "            map_sum += self.compute_map_given_conf(y_pred, y_true, conf)\n",
        "\n",
        "        return torch.tensor(map_sum / self.num_conf)\n",
        "\n",
        "\n",
        "def minimum_bounding_box(points: List[List[float]]) -> Tuple[float, float, float, float]:\n",
        "    x_min = min(p[0] for p in points)\n",
        "    y_min = min(p[1] for p in points)\n",
        "    x_max = max(p[0] for p in points)\n",
        "    y_max = max(p[1] for p in points)\n",
        "\n",
        "    return x_min, y_min, x_max, y_max\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfkIFx1ylP7Q"
      },
      "source": [
        "#### Объявляем конфиг для object_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCjuuZnqlXs3"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"dataset_path\": \"train/object_detection\",\n",
        "  \"model_log_interval\": 1,\n",
        "  \"data\": {\n",
        "    \"class_ids\": {\n",
        "      \"door\": 1,\n",
        "      \"door_balcony\": 1\n",
        "    }\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"image_size\": 512\n",
        "  },\n",
        "  \"training\": {\n",
        "    \"num_epochs\": 2,\n",
        "    \"batch_size\": 16,\n",
        "    \"update_every_n_batches\": 1,\n",
        "    \"device\": \"cpu\",\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_workers\": 0\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сборка результата сегментации и детекции"
      ],
      "metadata": {
        "id": "tlW5JO67kHhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_pred_with_boxes(image: np.ndarray, \n",
        "                         preds: Dict[str, np.ndarray] = None):\n",
        "\n",
        "    image = image.copy()\n",
        "\n",
        "    num_boxes = len(preds['boxes'])\n",
        "\n",
        "    for i in range(num_boxes):\n",
        "        x_min, y_min, x_max, y_max = preds['boxes'][i]\n",
        "\n",
        "        cv2.rectangle(img=image,\n",
        "                      pt1=(int(x_min), int(y_min)),\n",
        "                      pt2=(int(x_max), int(y_max)),\n",
        "                      color=3,\n",
        "                      thickness=-1)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "RjU0vq9jRRmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_dir = '/content/drive/MyDrive/experiment/'\n",
        "transform = get_test_augmentation()\n",
        "\n",
        "# Загрузка модели сегментации окон\n",
        "model_window = smp.FPN(encoder_name='resnet50', classes=24)\n",
        "model_window.load_state_dict(torch.load(\"/content/drive/MyDrive/model/first_model/model_window_1\", map_location=torch.device('cpu')))\n",
        "model_window.eval()\n",
        "\n",
        "# Загрузка модели сегментации стен\n",
        "model_wall = smp.FPN(encoder_name='resnet50', classes=24)\n",
        "model_wall.load_state_dict(torch.load(\"/content/drive/MyDrive/model/first_model/model_wall_1\", map_location=torch.device('cpu')))\n",
        "model_wall.eval()\n",
        "\n",
        "# Загрузка модели детекции дверей\n",
        "model_door = initialize_model(1 + max(config['data']['class_ids'].values()),\n",
        "                             config['model']['image_size'], config['model']['image_size'], [0, 0, 0], [1, 1, 1],\n",
        "                             config['training']['device'])\n",
        "model_door.load_state_dict(torch.load(\"/content/drive/MyDrive/model/model_obj_door_1\",  map_location=torch.device('cpu')))\n",
        "class_ids = config['data']['class_ids']\n",
        "reverse_classes_map = {v: k for k, v in class_ids.items()}\n",
        "transform_door = ToTensorV2()\n",
        "model_door.eval()\n",
        "\n",
        "# Прогресс\n",
        "prgBar = IntProgress(min = 0, max = len(os.listdir(folder_dir))) # Создаем прогрессбар\n",
        "display(prgBar) # Выводим прогрессбар на экран\n",
        "\n",
        "for images in os.listdir(folder_dir):\n",
        "    ##Загрузка тестового изображения\n",
        "    image = cv2.imread(folder_dir+images)\n",
        "    converted_image = convert_image(image)\n",
        "\n",
        "    ## Применение сегментации\n",
        "   \n",
        "    image = transform(image=converted_image)['image']\n",
        "    image = tensor_from_rgb_image(image)\n",
        "    image = image.view((1, 3, image.shape[1], image.shape[2]))\n",
        "\n",
        "    ## Получение предсказания по тестовому изображениям\n",
        "    with torch.no_grad():\n",
        "        predict_window = model_window(image)\n",
        "        predict_wall = model_wall(image)\n",
        "\n",
        "    predict_window = torch.sigmoid(predict_window) > 0.2\n",
        "    window = mask_to_grayscale(predict_window[0])\n",
        "\n",
        "    predict_wall = torch.sigmoid(predict_wall) > 0.2\n",
        "    wall = mask_to_grayscale(predict_wall[0])\n",
        "\n",
        "    ## Объединение масок и установка класса (1-стены, 2-окна, 3-двери)\n",
        "    wall[np.where(wall == 255)] = 1\n",
        "    wall[np.where(window == 255)] = 2\n",
        "    \n",
        "\n",
        "    ## Детекция дверей\n",
        "    image = cv2.imread(folder_dir+images).astype(np.float32) / 255\n",
        "    bytearray_ = np.asarray(bytearray(image), dtype=np.uint8)\n",
        "    x = image\n",
        "    x = transform_door(image=x)['image']\n",
        "    with torch.no_grad():\n",
        "        y_pred = model_door([torch.tensor(x, device=config['training']['device'])])[0]\n",
        "        y_pred = {key: y_pred[key].cpu().numpy() for key in y_pred}\n",
        "        y_pred['labels'] = np.array([reverse_classes_map[label] for label in y_pred['labels']])\n",
        "\n",
        "    ## Возвращение оригинального размера изображения\n",
        "    result_img = cv2.resize(wall, (cv2.imread(folder_dir+images).shape[1], cv2.imread(folder_dir+images).shape[0]))\n",
        "\n",
        "    ## Объединение сегментации и детекции\n",
        "    result_img = draw_pred_with_boxes(result_img, y_pred)\n",
        "\n",
        "    ## Сохранение файла\n",
        "    cv2.imwrite(\"/content/drive/MyDrive/test_result/\" + images, result_img)\n",
        "\n",
        "    ## Прогресс\n",
        "    prgBar.value = prgBar.value + 1 # Двигаем \"полоску\""
      ],
      "metadata": {
        "id": "ifthfV3oN467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3cb8adf81bb74784b87e2611273a8bd5",
            "79139666d6584c6abc533f0503a4dc31",
            "d9539239b6bd462f826816cbd7a4cd45"
          ]
        },
        "outputId": "89ba89a5-efeb-473b-9e2d-c90651c0ce70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, max=1)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cb8adf81bb74784b87e2611273a8bd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = cv2.imread('/content/drive/MyDrive/test_result/test.png', 0)\n",
        "plt.imshow(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-NPAHoHo6DD_",
        "outputId": "2148f71c-a5ed-47b9-ee7c-1ffd52f57d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(850, 800)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZnIyPcTgle1F",
        "RTjlo03_R9tZ",
        "HfkIFx1ylP7Q",
        "ElCb7TxqoOYD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3cb8adf81bb74784b87e2611273a8bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79139666d6584c6abc533f0503a4dc31",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9539239b6bd462f826816cbd7a4cd45",
            "value": 1
          }
        },
        "79139666d6584c6abc533f0503a4dc31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9539239b6bd462f826816cbd7a4cd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}